box()
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
set.seed(4268)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
set.seed(0) # for reproducibility
tr = sample.int(nrow(M),nrow(M)/2)
trte=rep(1,nrow(M))
trte[tr]=0
Mdf=data.frame(M,"istest"=as.factor(trte))
k_values = 1:30
test.e <- vector(mode = "numeric", length = 30)
train.e <- vector(mode = "numeric", length = 30)
for(k in k_values){
test.e[k] = mean(Mdf[-tr,]$y != knn(Mdf[tr,], Mdf[-tr,],Mdf[tr,]$y, k = k, l = 0))
train.e[k] = mean(Mdf[tr,]$y != knn(Mdf[tr,], Mdf[tr,],Mdf[tr,]$y, k = k, l = 0))
}
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k
#k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k = 22
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
par(mar=rep(2,4), mgp = c(1, 1, 0))
np = 300
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
size = 100
xnew = apply(M[tr,-1], 2, function(X) seq(min(X), max(X), length.out=size))
grid = expand.grid(xnew[,1], xnew[,2])
grid.yhat = knn(M[tr,-1], M[tr,1], k=k, test=grid)
np = 300
par(mar=rep(2,4), mgp = c(1, 1, 0))
contour(xnew[,1], xnew[,2], z = matrix(grid.yhat, size), levels=.5,
xlab=expression("x"[1]), ylab=expression("x"[2]), axes=FALSE,
main = paste0(k,"-nearest neighbors"), cex=1.2, labels="")
points(grid, pch=".", cex=1, col=grid.yhat)
points(M[1:np,-1], col=factor(M[1:np,1]), pch = 1, lwd = 1.5)
legend("topleft", c("Player 1 wins", "Player 2 wins"),
col=c("red", "black"), pch=1)
box()
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
set.seed(0)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
cv.se <- vector(mode = "numeric", length = 30)
for(i in 1:30){
for(j in 1:5){
cv.se[i] = cv.se[i] + (cv[j,i]- cv.e[i]) ** 2
}
cv.se[i] = sqrt(1/n * cv.se[i])
}
cv.se
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k
set.seed(986)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
set.seed(87)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
cv.se <- vector(mode = "numeric", length = 30)
for(i in 1:30){
for(j in 1:5){
cv.se[i] = cv.se[i] + (cv[j,i]- cv.e[i]) ** 2
}
cv.se[i] = sqrt(1/n * cv.se[i])
}
cv.se
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k
set.seed(368)
set.seed(368)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
cv.se <- vector(mode = "numeric", length = 30)
for(i in 1:30){
for(j in 1:5){
cv.se[i] = cv.se[i] + (cv[j,i]- cv.e[i]) ** 2
}
cv.se[i] = sqrt(1/n * cv.se[i])
}
cv.se
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k
set.seed(1895)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
cv.e <- vector(mode = "numeric", length = 30)
print(cv)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
cv.se <- vector(mode = "numeric", length = 30)
for(i in 1:30){
for(j in 1:5){
cv.se[i] = cv.se[i] + (cv[j,i]- cv.e[i]) ** 2
}
cv.se[i] = sqrt(1/n * cv.se[i])
}
cv.se
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k
set.seed(12)
ks = 1:30 # Choose K from 1 to 30.
idx = createFolds(M[tr,1], k=5) # Divide the training data into 5 folds.
# "Sapply" is a more efficient for-loop.
# We loop over each fold and each value in "ks"
# and compute error rates for each combination.
# All the error rates are stored in the matrix "cv",
# where folds are rows and values of $K$ are columns.
cv = sapply(ks, function(k){
sapply(seq_along(idx), function(j) {
yhat = class::knn(train=M[tr[ -idx[[j]] ], -1],
cl=M[tr[ -idx[[j]] ], 1],
test=M[tr[ idx[[j]] ], -1], k = k)
mean(M[tr[ idx[[j]] ], 1] != yhat)
})
})
print(cv)
cv.e <- vector(mode = "numeric", length = 30)
for(i in 1:30){
cv.e[i] = mean(cv[,i])
}
cv.e
k.min <- which.min(cv.e)
k.min
cv.se <- vector(mode = "numeric", length = 30)
for(i in 1:30){
for(j in 1:5){
cv.se[i] = cv.se[i] + (cv[j,i]- cv.e[i]) ** 2
}
cv.se[i] = sqrt(1/n * cv.se[i])
}
cv.se
library(colorspace)
co = rainbow_hcl(3)
par(mar=c(4,4,1,1)+.1, mgp = c(3, 1, 0))
plot(ks, cv.e, type="o", pch = 16, ylim = c(0, 0.7), col = co[2],
xlab = "Number of neighbors", ylab="Misclassification error")
arrows(ks, cv.e-cv.se, ks, cv.e+cv.se, angle=90, length=.03, code=3, col=co[2])
lines(ks, train.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[3])
lines(ks, test.e, type="o", pch = 16, ylim = c(0.5, 0.7), col = co[1])
legend("topright", legend = c("Test", "5-fold CV", "Training"), lty = 1, col=co)
k = tail(which(cv.e < cv.e[k.min] + cv.se[k.min]), 1)
k
K=k
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
library(pROC)
install.packages("pROC")
install.packages("pROC")
K=k
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
library(pROC)
# now you use predictor=KNNprob and response=M[-tr,1]
knitr::opts_chunk$set(echo = TRUE)
library(GLMsData)
data("lungcap")
lungcap$Htcm=lungcap$Ht*2.54
modelA = lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelA)
K=30
# knn with prob=TRUE outputs the probability of the winning class
# therefore we have to do an extra step to get the probability of player 1 winning
KNNclass=class::knn(train=M[tr,-1], cl=M[tr,1], test=M[-tr,-1], k = K,prob=TRUE)
KNNprobwinning=attributes(KNNclass)$prob
KNNprob= ifelse(KNNclass == "0", 1-KNNprobwinning, KNNprobwinning)
values=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X=values$X
dim(X)
x0=values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
values=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X=values$X
dim(X)
x0=values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
sqbias=function(lambda,X,x0,beta)
{
p=dim(X)[2]
value= #HERE YOU FILL IN
return(value)
}
thislambda=seq(0,2,length=500)
sqbiaslambda=rep(NA,length(thislambda))
for (i in 1:length(thislambda)) sqbiaslambda[i]=sqbias(thislambda[i],X,x0,beta)
knitr::opts_chunk$set(echo = TRUE)
library(GLMsData)
data("lungcap")
lungcap$Htcm=lungcap$Ht*2.54
modelA = lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data=lungcap)
summary(modelA)
library(ggplot2)
# residuls vs fitted
ggplot(modelA, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
labs(x = "Fitted values", y = "Standardized residuals",
title = "Fitted values vs. standardized residuals for model A",
subtitle = deparse(modelA$call))
# qq-plot of residuals
ggplot(modelA, aes(sample = .stdresid)) +
stat_qq(pch = 19) +
geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
labs(x = "Theoretical quantiles", y = "Standardized residuals",
title = "Normal Q-Q", subtitle = deparse(modelA$call))
# normality test
library(nortest)
ad.test(rstudent(modelA))
modelB = lm(FEV ~ Age + Htcm + Gender + Smoke, data=lungcap)
ggplot(modelB, aes(.fitted, .stdresid)) + geom_point(pch = 21) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_smooth(se = FALSE, col = "red", size = 0.5, method = "loess") +
labs(x = "Fitted values", y = "Standardized residuals",
title = "Fitted values vs. standardized residuals for modelB",
subtitle = deparse(modelB$call))
confint(modelA, level=0.99)
new = data.frame(Age=16, Htcm=170, Gender="M", Smoke=0)
prelog <- predict.lm(modelA, new, interval = "prediction", level = 0.95)
prediction <- exp(prelog[1])
low <- exp(prelog[2])
upp <- exp(prelog[3])
cat('Predicted "forced expiratory colume" (FEV) = ', prediction, '. \n 95% confidence intercal = [',low, ',',upp,']' )
library(class)# for function knn
library(caret)# for confusion matrices
raw = read.csv("https://www.math.ntnu.no/emner/TMA4268/2019v/data/tennis.csv")
M = na.omit(data.frame(y=as.factor(raw$Result),
x1=raw$ACE.1-raw$UFE.1-raw$DBF.1,
x2=raw$ACE.2-raw$UFE.2-raw$DBF.2))
set.seed(4268) # for reproducibility
tr = sample.int(nrow(M),nrow(M)/2)
trte=rep(1,nrow(M))
trte[tr]=0
Mdf=data.frame(M,"istest"=as.factor(trte))
k_values = 1:30
test.e <- vector(mode = "numeric", length = 30)
train.e <- vector(mode = "numeric", length = 30)
for(k in k_values){
test.e[k] = mean(Mdf[-tr,]$y != knn(Mdf[tr,], Mdf[-tr,],Mdf[tr,]$y, k = k, l = 0))
train.e[k] = mean(Mdf[tr,]$y != knn(Mdf[tr,], Mdf[tr,],Mdf[tr,]$y, k = k, l = 0))
}
qplot(k_values, test.e)
qplot(k_values, train.e)
# here you write your code
values=dget("https://www.math.ntnu.no/emner/TMA4268/2019v/data/BVtradeoffvalues.dd")
X=values$X
dim(X)
x0=values$x0
dim(x0)
beta=values$beta
dim(beta)
sigma=values$sigma
sigma
setwd("~/LineÃ¦re statistiske modeller/LinStat3")
install.packages("FrF2")
knitr::opts_chunk$set(echo = TRUE)
library(FrF2)
plan <- FrF2(nruns = 32, nfactors=4, randomize = T)
library(FrF2)
plan <- FrF2(nruns = 16, nfactors=4, randomize = T)
plan
plan
plan <- FrF2(nruns = 32, nfactors=4, randomize = T)
plan
plan <- FrF2(nruns = 32, nfactors=4, randomize = F)
plan
plan <- FrF2(nruns = 16, nfactors=4, randomize = T)
plan
plan
plan <- FrF2(nruns = 32, nfactors=4, randomize = T)
plan
plan <- FrF2(nruns = 32, nfactors=4, randomize = F)
plan
